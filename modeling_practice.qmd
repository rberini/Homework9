---
title: "Modeling Practice"
author: "Robert Berini"
format: html
editor: visual
execute: 
  warning: false
---

## Load required packages

```{r}
library(tidyverse)
library(tidymodels)
library(baguette)
library(janitor)
library(naniar)
library(skimr)
library(pastecs)
library(vip)
options(scipen = 999, digits = 2)
tidymodels_prefer()
```

## Read data

```{r}
seoul_bike_data <- read_csv("https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv",
         locale = locale(encoding = "Latin1"))
```

## Check and manipulate the data

Check for any missing values.

```{r}
vis_miss(seoul_bike_data)
```

Explore columns names, column types, and values.

```{r}
glimpse(seoul_bike_data)
```

Generate basic summary statistics for numeric columns and check the unique values for the categorical variables.

```{r}
skim(seoul_bike_data)
```

Simplify column names.

```{r}
names(seoul_bike_data) <- str_remove(names(seoul_bike_data), "\\s*\\([^\\)]+\\)")
```

Convert the `Date` column into an actual date. Turn the character variables (`Seasons`, `Holiday`, and `Functioning Day`) into factors. Rename the all variables to have easy to use names in snake case.

```{r}
seoul_bike_data <-
  seoul_bike_data |>
  mutate(Date = dmy(Date)) |>
  mutate(across(where(is.character), as.factor)) |>
  clean_names()

seoul_bike_data
```

## Exploratory data analysis

Generate summary statistics across variables.

```{r}
summary(seoul_bike_data)
```

```{r}
skim(seoul_bike_data)
```

Explore `rented_bike_count` by the `functioning_day` variable.

```{r}
seoul_bike_data |>
  group_by(functioning_day) |>
  select(rented_bike_count) |>
  skim()
```

There appear to be no bike rentals on non-functioning days. This implies the business is closed on these days and should be removed from the final data used for analysis.

Create an object that captures all the weather variables.

```{r}
weather_vars <- names(seoul_bike_data[4:9])
weather_vars
```

To simplify analysis, summarize across the hours so that each day has one observation associated with it:

-   `group_by()` the `date`, `seasons`, and `holiday` variables
-   find the `sum` of the `rented_bike_count`, `rainfall`, and `snowfall` variables
-   find the `mean` of all the weather related variables

```{r}
new_seoul_bike_data <-
  seoul_bike_data |>
  filter(functioning_day == "Yes") |>
  group_by(date, seasons, holiday) |>
  summarise(across(c(rented_bike_count, rainfall, snowfall), sum),
            across(all_of(weather_vars), mean))

new_seoul_bike_data
```

Recreate basic summary statistics.

```{r}
summary(new_seoul_bike_data)
```

```{r}
stat.desc(new_seoul_bike_data[,4:ncol(new_seoul_bike_data)], basic = F)
```

Report correlation between numeric variables.

```{r}
cor(new_seoul_bike_data[,4:ncol(new_seoul_bike_data)])
```

Create some plots to explore relationships.

```{r}
new_seoul_bike_data |>
  ggplot(aes(x = temperature, y = rented_bike_count)) +
  geom_point(aes(color = holiday)) +
  facet_wrap(~ seasons) +
  ggtitle("Relationship Between Rented Bike Count and Solar Radiation",
          subtitle = "Considering Effects of Season and Holiday") +
  xlab("Temperature") +
  ylab("Rented Bike Count")
```

```{r}
new_seoul_bike_data |>
  ggplot(aes(x = wind_speed, y = rented_bike_count)) +
  geom_point() +
  facet_wrap(~ seasons) +
  ggtitle("Relationship Between Rented Bike Count and Wind Speed",
          subtitle = "Considering Effects of Season") +
  xlab("Wind Speed") +
  ylab("Rented Bike Count")
```

```{r}
new_seoul_bike_data |>
  ggplot(aes(x = solar_radiation, y = rented_bike_count)) +
  geom_point() +
  facet_wrap(~ seasons) +
  ggtitle("Relationship Between Rented Bike Count and Solar Radiation",
          subtitle = "Considering Effects of Season") +
  xlab("Solar Radiation") +
  ylab("Rented Bike Count")
```

```{r}
new_seoul_bike_data |>
  mutate(snowfall = factor(if_else(snowfall == 0, "No Snow", "Snow"))) |>
  ggplot(aes(x = snowfall, y = rented_bike_count)) +
  geom_boxplot() +
  ggtitle("Distribution of Rented Bike Count for Snowy Versus Non-Snowy Days") +
  xlab("Snowfall") +
  ylab("Rented Bike Count")
```

```{r}
new_seoul_bike_data |>
  mutate(rainfall = factor(if_else(rainfall == 0, "No Rain", "Rain"))) |>
  ggplot(aes(x = rainfall, y = rented_bike_count)) +
  geom_boxplot() +
  ggtitle("Distribution of Rented Bike Count for Rainy Versus Non-Rainy Days") +
  xlab("Rainfall") +
  ylab("Rented Bike Count")
```

```{r}
new_seoul_bike_data |>
  mutate(weekpart = factor(if_else(wday(date, label = T) %in% c("Sun", "Sat"), "Weekend", "Weekday"))) |>
  ggplot(aes(x = weekpart, y = rented_bike_count)) +
  geom_boxplot() +
  ggtitle("Distribution of Rented Bike Count for Weekdays Versus Weekends") +
  xlab("Part of Week") +
  ylab("Rented Bike Count")
```

```{r}
new_seoul_bike_data |>
  ggplot(aes(x = holiday, y = rented_bike_count)) +
  geom_boxplot() +
  ggtitle("Distribution of Rented Bike Count for Holidays Versus Non-Holidays") +
  xlab("Holiday") +
  ylab("Rented Bike Count")
```

## Split the data

Split the data into a training and test set (75/25 split). Use the `strata` argument to stratify the split on the `seasons` variable.

```{r}
set.seed(1017)
bike_split <- initial_split(new_seoul_bike_data, prop = 0.75, strata = "seasons")
bike_train <- training(bike_split)
bike_test <- testing(bike_split)
```

On the training set, create a 10 fold CV split.

```{r}
bike_10_fold <- vfold_cv(bike_train, 10)
```

## Create recipes

For the 1st recipe:

-   ignore the date variable for modeling, but use it to create a weekday/weekend (factor) variable called `weekpart`
-   standardize the numeric variables
-   create dummy variables for the `seasons`, `holiday`, and new `weekpart` variable

```{r}
bike_rec_1 <-
  recipe(rented_bike_count ~ ., data = bike_train) |>
  step_date(date, features = "dow") |>
  step_mutate(weekpart = factor(if_else(date_dow %in% c("Sun", "Sat"), "Weekend", "Weekday"))) |>
  step_rm(date, date_dow) |>
  step_normalize(all_numeric(), -all_outcomes()) |>
  step_dummy(seasons, holiday, weekpart)
```

```{r}
bike_rec_1 |>
  prep(training = bike_train) |>
  bake(bike_train)
```

For the 2nd recipe:

-   do the same steps as above
-   add in interactions between `seasons` and `holiday`, `seasons` and `temperature`, and `temperature` and `rainfall`

```{r}
bike_rec_2 <-
  bike_rec_1 |>
  step_interact(~ starts_with("holiday"):starts_with("seasons") +
                  temperature:starts_with("seasons") +
                  temperature:rainfall)
```

```{r}
bike_rec_2 |>
  prep(training = bike_train) |>
  bake(bike_train)
```

For the 3rd recipe:

-   do the same as the 2nd recipe
-   add in quadratic terms for each numeric predictor

```{r}
bike_rec_3 <-
  recipe(rented_bike_count ~ ., data = bike_train) |>
  step_date(date, features = "dow") |>
  step_mutate(weekpart = factor(if_else(date_dow %in% c("Sun", "Sat"), "Weekend", "Weekday"))) |>
  step_rm(date, date_dow) |>
  step_normalize(all_numeric(), -all_outcomes()) |>
  step_poly(all_numeric(), -all_outcomes(), degree = 2, keep_original_cols = T) |>
  step_rm(ends_with("poly_1")) |>
  step_dummy(seasons, holiday, weekpart) |>
  step_interact(~ starts_with("holiday"):starts_with("seasons") +
                  temperature:starts_with("seasons") +
                  temperature:rainfall)
```

```{r}
bike_rec_3 |>
  prep(training = bike_train) |>
  bake(bike_train)
```

## Fit MLR Models

Set up linear model fit to use the `“lm”` engine.

```{r}
lm_mod <-
  linear_reg() |>
  set_engine("lm")
```

Fit models for recipe 1 using 10 fold CV via `fit_resamples()` and consider the training set CV error.

```{r}
bike_wfl_1 <-
  workflow() |>
  add_recipe(bike_rec_1) |>
  add_model(lm_mod)
```

```{r}
bike_cv_fits_1 <-
  bike_wfl_1 |>
  fit_resamples(bike_10_fold)
```

```{r}
bike_cv_1_metrics <-
  bike_cv_fits_1 |>
  collect_metrics()
```

Repeat process for recipe 2.

```{r}
bike_wfl_2 <-
  workflow() |>
  add_recipe(bike_rec_2) |>
  add_model(lm_mod)
```

```{r}
bike_cv_fits_2 <-
  bike_wfl_2 |>
  fit_resamples(bike_10_fold)
```

```{r}
bike_cv_2_metrics <-
  bike_cv_fits_2 |>
  collect_metrics()
```

Repeat process for recipe 3.

```{r}
bike_wfl_3 <-
  workflow() |>
  add_recipe(bike_rec_3) |>
  add_model(lm_mod)
```

```{r}
bike_cv_fits_3 <-
  bike_wfl_3 |>
  fit_resamples(bike_10_fold)
```

```{r}
bike_cv_3_metrics <-
  bike_cv_fits_3 |>
  collect_metrics()
```

Consider the training set CV error across recipes to choose a best model.

```{r}
rbind(bike_cv_1_metrics, bike_cv_2_metrics, bike_cv_3_metrics) |>
  filter(.metric == "rmse") |>
  mutate(model = c("Recipe 1", "Recipe 2", "Recipe 3")) |>
  select(model, "mean_rmse" = mean, n, std_err)
```

```{r}
bike_best_fit <-
  bike_wfl_3 |>
  fit(bike_train)
```

```{r}
bike_best_fit |>
  tidy()
```

Using the best model, fit the model to the entire training data set using the `last_fit()` function. Compute the RMSE metric on the test set.

```{r}
bike_wfl_3 |>
  last_fit(split = bike_split) |>
  collect_metrics()
```

Obtain the final model (fit on the entire training set) coefficient table using `extract_fit_parsnip()` and `tidy()`.

```{r}
extract_fit_parsnip(
  bike_wfl_3 |>
  last_fit(split = bike_split)
) |>
  tidy() |>
  print(n=Inf)
```

```{r}
mlr_metrics <-
  bike_wfl_3 |>
  last_fit(split = bike_split) |>
  collect_metrics()
```

## Fit LASSO Model

```{r}
lasso_spec <-
  linear_reg(penalty = tune(), mixture = 1) |>
  set_engine("glmnet")
```

```{r}
bike_lasso_wfl <-
  workflow() |>
  add_recipe(bike_rec_3) |>
  add_model(lasso_spec)
```

```{r}
set.seed(1017)

bike_lasso_grid <-
  bike_lasso_wfl |>
  tune_grid(resamples = bike_10_fold,
            grid = grid_regular(penalty(range = c(-4, 2)), levels = 500)) 
```

```{r}
bike_lasso_grid |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  arrange(mean)
```

```{r}
bike_lasso_grid |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_line() +
  scale_x_log10() +
  scale_color_viridis_d()
```

```{r}
lowest_rmse <- 
  bike_lasso_grid |>
  select_best(metric = "rmse")
lowest_rmse
```

```{r}
bike_lasso_final <- 
  bike_lasso_wfl |>
  finalize_workflow(lowest_rmse) |>
  fit(bike_train)

tidy(bike_lasso_final) |>
  print(n=Inf)
```

```{r}
bike_lasso_wfl |>
  finalize_workflow(lowest_rmse) |>
  last_fit(split = bike_split) |>
  collect_metrics()
```

```{r}
bike_lasso_wfl |>
  finalize_workflow(lowest_rmse) |>
  last_fit(split = bike_split) |>
  extract_fit_parsnip() |>
  tidy() |>
  print(n=Inf)
```

```{r}
lasso_metrics <-
  bike_lasso_wfl |>
  finalize_workflow(lowest_rmse) |>
  last_fit(split = bike_split) |>
  collect_metrics()
```

## Fit Regression Tree Model

```{r}
tree_mod <- decision_tree(tree_depth = tune(),
                          min_n = 10,
                          cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("regression")
```

```{r}
bike_tree_wfl <- 
  workflow() |>
  add_recipe(bike_rec_1) |>
  add_model(tree_mod)
```

```{r}
set.seed(1017)

tree_fits <-
  bike_tree_wfl |> 
  tune_grid(resamples = bike_10_fold, grid = 50)
```

```{r}
tree_fits |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  arrange(mean)
```

```{r}
tree_fits |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d()
```

```{r}
tree_best_params <-
  tree_fits |>
  select_best(metric = "rmse")
tree_best_params
```

```{r}
bike_tree_final_fit <-
  bike_tree_wfl |>
  finalize_workflow(tree_best_params) |>
  last_fit(split = bike_split)

bike_tree_final_fit |>
  collect_metrics()
```

```{r}
bike_tree_final_fit |>
  extract_workflow() |>
  extract_fit_engine() |>
  rpart.plot::rpart.plot(roundint = F)
```

```{r}
tree_metrics <-
  bike_tree_final_fit |>
  collect_metrics()
```

## Fit Bagged Tree Model

```{r}
bag_mod <- bag_tree(tree_depth = tune(),
                          min_n = 10,
                          cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("regression") |>
  translate()
```

```{r}
bike_bag_wfl <- 
  workflow() |>
  add_recipe(bike_rec_1) |>
  add_model(bag_mod)
```

```{r}
set.seed(1017)

bag_fits <-
  bike_bag_wfl |> 
  tune_grid(resamples = bike_10_fold, grid = 50)
```

```{r}
bag_fits |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  arrange(mean)
```

```{r}
bag_fits |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d()
```

```{r}
bag_best_params <-
  bag_fits |>
  select_best(metric = "rmse")
bag_best_params
```

```{r}
bike_bag_final_fit <-
  bike_bag_wfl |>
  finalize_workflow(bag_best_params) |>
  last_fit(split = bike_split)

bike_bag_final_fit |>
  collect_metrics()
```


```{r}
bike_bag_final_model <- extract_fit_engine(bike_bag_final_fit) 
bike_bag_final_model$imp |>
  mutate(term = factor(term, levels = term)) |>
  ggplot(aes(x = reorder(term, value), y = value)) + 
  geom_bar(stat ="identity") +
  coord_flip() +
  theme(axis.title.y = element_blank()) +
  ylab("Importance")
```

```{r}
bag_metrics <-
  bike_bag_final_fit |>
  collect_metrics()
```

## Fit Random Forest Model

```{r}
rf_mod <- rand_forest(mtry = tune(),
                      min_n = 10,
                      trees = tune()) |>
  set_engine("ranger", importance = "impurity") |>
  set_mode("regression")
```

```{r}
bike_rf_wfl <- 
  workflow() |>
  add_recipe(bike_rec_1) |>
  add_model(rf_mod)
```

```{r}
set.seed(1017)

rf_fits <-
  bike_rf_wfl |> 
  tune_grid(resamples = bike_10_fold, grid = 50)
```

```{r}
rf_fits |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  arrange(mean)
```

```{r}
rf_best_params <-
  rf_fits |>
  select_best(metric = "rmse")
rf_best_params
```

```{r}
bike_rf_final_fit <-
  bike_rf_wfl |>
  finalize_workflow(rf_best_params) |>
  last_fit(split = bike_split)

bike_rf_final_fit |>
  collect_metrics()
```

```{r}
bike_rf_final_model <- extract_fit_engine(bike_rf_final_fit) 
bike_rf_final_model |>
  vip(num_features = 20)
```


```{r}
rf_metrics <-
  bike_rf_final_fit |>
  collect_metrics()
```

## Select the Best Model

```{r}
rbind(mlr_metrics, lasso_metrics, tree_metrics, bag_metrics, rf_metrics) |>
  filter(.metric == "rmse") |>
  mutate(model = c("MLR", "LASSO", "Tree", "Bagged Tree", "Random Forest")) |>
  select(model, "mean_rmse" = .estimate)
```
